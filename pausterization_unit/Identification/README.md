# Koopman Model for Pasteurization Unit - Inference Guide

This directory contains the trained Koopman model for the pasteurization unit system and the inference interface to use it.

## Quick Start

```python
import numpy as np
import baseline_inference as model

# 1. Initialize the model (loads weights and scalers)
model.init()

# 2. Set current state from measurements [T1, T2, T4]
y_current = np.array([[75.5], [80.2], [78.9]])  # Temperatures in °C
model.get_x(y_current)

# 3. Apply control and predict next state [u1, u2, u3]
u_control = np.array([[0.5], [0.3], [0.7]])
y_next = model.y_plus(u_control)

print(f"Current: {y_current.flatten()}")
print(f"Predicted next: {y_next.flatten()}")
```

**See `example_usage.py` for more detailed examples.**

## Overview

The Koopman model learns a linear representation of the nonlinear pasteurization unit dynamics in a lifted latent space. The model architecture consists of:

- **Encoder**: Neural network that maps output measurements to a latent representation
- **Koopman Operator**: Linear dynamics in latent space (A matrix)
- **Input Encoder**: Linear mapping from control inputs to latent space (B matrix)
- **Decoder**: Neural network that maps latent states back to output space

### System Dynamics

In latent space, the system follows linear dynamics:

```
x[k+1] = A @ x[k] + B @ u[k]
y[k] = decoder(x[k])
```

Where:

- `x` is the latent state (26-dimensional)
- `u` is the control input (3-dimensional)
- `y` is the system output (3-dimensional)

## Model Specifications

### Input/Output Dimensions

- **Outputs (ny=3)**:

  - T1: Temperature at point 1 [°C]
  - T2: Temperature at point 2 [°C]
  - T4: Temperature at point 4 [°C]

- **Inputs (nu=3)**:

  - u1: Control input 1
  - u2: Control input 2
  - u3: Control input 3

- **Latent States (nz=26)**: Internal representation dimension

### Network Architecture

- **Encoder Layers**: [12, 24, 36] with ReLU activation
- **Decoder Layers**: [36, 24, 12] with ELU activation
- **Prediction Horizon**: 80 time steps
- **Batch Size during Training**: 80

## Required Files

The following files must be present in the `../data/` directory for the model to work:

### 1. Trained Model Weights

- **File**: `model_C_False.pth`
- **Description**: PyTorch state dict containing all trained neural network weights
- **Generated by**: Training notebook (`example_training.ipynb`)

### 2. Output Scaler

- **File**: `scaler.pkl`
- **Description**: Sklearn StandardScaler fitted on output data (T1, T2, T4)
- **Purpose**: Normalizes outputs to zero mean and unit variance
- **Generated by**: Training notebook using `train['Y']` data

### 3. Input Scaler

- **File**: `scalerU.pkl`
- **Description**: Sklearn StandardScaler fitted on input data (u1, u2, u3)
- **Purpose**: Normalizes inputs to zero mean and unit variance
- **Generated by**: Training notebook using `train['U']` data

### 4. System Matrices (Optional)

- **Files**: `A_C_False.npy`, `B_C_False.npy`, `C_C_False.npy`
- **Description**: Extracted linear system matrices
- **Note**: These are extracted automatically during inference, but can be pre-loaded if needed

## Training Data

The model was trained on experimental data from the pasteurization unit:

### Training Dataset

- **Source**: `.mat` files in `../experimental_data/`
  - `T1_ident_2.mat`, `T2_ident_2.mat`, `T4_ident_2.mat` (outputs)
  - `u1_ident_2.mat`, `u2_ident_2.mat`, `u3_ident_2.mat` (inputs)

### Test Dataset

- **Source**: `.mat` files in `../experimental_data/`
  - `T1_ident_1.mat`, `T2_ident_1.mat`, `T4_ident_1.mat` (outputs)
  - `u1_ident_1.mat`, `u2_ident_1.mat`, `u3_ident_1.mat` (inputs)

### Data Characteristics

- **Sampling Time**: 1 second
- **Data Format**: Time series of temperature measurements and control inputs
- **Preprocessing**: Both inputs and outputs are standardized using `StandardScaler`

### Training Configuration

- **Optimizer**: Adam with learning rate 0.001
- **Epochs**: 3500 (with early stopping)
- **Loss Components**:
  - Output trajectory tracking loss (weight: 10.0)
  - One-step prediction loss (weight: 1.0)
  - Reconstruction loss (weight: 20.0)
  - Latent trajectory tracking loss (weight: 1.0)

## Installation Requirements

```bash
pip install torch numpy joblib scikit-learn scipy matplotlib
pip install neuromancer  # For model architecture
```

## Usage

### 1. Basic Initialization

```python
import numpy as np
import baseline_inference as model

# Initialize the model (must be called first)
model.init()
```

This will:

- Load the trained model weights from `model_C_False.pth`
- Load scalers from `scaler.pkl` and `scalerU.pkl`
- Extract A and B matrices from the trained model
- Print confirmation with matrix dimensions

### 2. Encode Output Measurements to Latent State

```python
# Provide current output measurements [T1, T2, T4] in °C
y_measurement = np.array([[75.5],   # T1
                          [80.2],   # T2
                          [78.9]])  # T4

# Encode to latent space (updates internal state)
model.get_x(y_measurement)

# Access the latent state
current_x = model.x  # Shape: [26, 1]
print(f"Current latent state: {current_x.shape}")
```

### 3. Decode Latent State to Output

```python
# Get output from latent state
y_decoded = model.get_y(model.x)
print(f"Decoded output: {y_decoded}")  # Shape: [3, 1]
print(f"T1: {y_decoded[0, 0]:.2f} °C")
print(f"T2: {y_decoded[1, 0]:.2f} °C")
print(f"T4: {y_decoded[2, 0]:.2f} °C")
```

### 4. Predict Next Step with Control Input

```python
# Define control inputs
u_control = np.array([[0.5],   # u1
                      [0.3],   # u2
                      [0.7]])  # u3

# Predict next output (also updates internal state)
y_next = model.y_plus(u_control)
print(f"Predicted next output: {y_next}")  # Shape: [3, 1]
```

### 5. Multi-Step Prediction

```python
# Simulate multiple steps ahead
n_steps = 10
control_sequence = np.random.rand(3, n_steps)  # Random control inputs

# Reset to initial condition
y_initial = np.array([[75.0], [80.0], [78.0]])
model.get_x(y_initial)

# Simulate forward
predictions = []
for k in range(n_steps):
    u_k = control_sequence[:, k].reshape(-1, 1)
    y_k = model.y_plus(u_k)
    predictions.append(y_k)

predictions = np.hstack(predictions)  # Shape: [3, n_steps]
```

### 6. Complete Example: Model Predictive Control Setup

```python
import numpy as np
import baseline_inference as model

# Initialize model
model.init()

# Set initial condition from measurement
y_current = np.array([[75.0], [80.0], [78.0]])  # Current temperatures
model.get_x(y_current)

# Define reference trajectory
y_ref = np.array([[77.0], [82.0], [80.0]])  # Desired temperatures

# Simple proportional control (replace with MPC optimizer)
error = y_ref - y_current
u_control = 0.1 * error  # Proportional gain

# Apply control and predict
y_next = model.y_plus(u_control)
print(f"Current output: {y_current.flatten()}")
print(f"Control applied: {u_control.flatten()}")
print(f"Predicted next output: {y_next.flatten()}")
```

## API Reference

### `init()`

Initialize the model and load all required files.

- **Returns**: None
- **Side Effects**: Sets global variables `problem`, `A`, `B`, `K`, `f_u`, `scaler`, `scalerU`
- **Prints**: Confirmation message with matrix dimensions

### `get_x(y)`

Encode output measurement to latent state.

- **Parameters**:
  - `y`: numpy array, shape `[ny, 1]` or `[ny,]` - Output measurements in original scale
- **Returns**: None
- **Side Effects**: Updates global variable `x` with encoded latent state

### `get_y(x)`

Decode latent state to output space.

- **Parameters**:
  - `x`: numpy array, shape `[nz, 1]` or `[nz,]` - Latent state
- **Returns**: numpy array, shape `[ny, 1]` - Output in original scale

### `y_plus(u)`

Predict next output given control input using Koopman dynamics.

- **Parameters**:
  - `u`: numpy array, shape `[nu, 1]` or `[nu,]` - Control input in original scale
- **Returns**: numpy array, shape `[ny, 1]` - Next output in original scale
- **Side Effects**: Updates global variable `x` with new latent state

## Model Properties

### Controllability and Observability

According to the training notebook analysis:

- **Controllability**: The system is fully controllable (rank 26/26)
- **Observability**: The system is fully observable (rank 26/26)

This means:

- All latent states can be controlled via the inputs
- All latent states can be inferred from the outputs

### Performance Metrics (from training)

- **MAE on test set** (excluding first 20 steps): 0.534 (scaled units)
- Training used a reconstruction loss with weight 20.0 to ensure accurate initial state reconstruction

## Troubleshooting

### FileNotFoundError: scaler.pkl or scalerU.pkl

**Solution**: Run the training notebook `example_training.ipynb` to generate these files.

### FileNotFoundError: model_C_False.pth

**Solution**: Complete the training in `example_training.ipynb` which saves the model.

### Shape mismatch errors

**Solution**: Ensure inputs are numpy arrays with correct dimensions:

- Outputs `y`: shape `[3, 1]` or `[3,]`
- Inputs `u`: shape `[3, 1]` or `[3,]`
- Latent states `x`: shape `[26, 1]` or `[26,]`

### Model predictions are in scaled units

**Solution**: The `get_y()` and `y_plus()` functions automatically handle inverse scaling. If you're accessing predictions directly from the model nodes, you may need to manually apply inverse scaling.

## Directory Structure

```
pausterization_unit/
├── Identification/
│   ├── baseline_inference.py    # This inference interface
│   ├── example_training.ipynb   # Training notebook
│   └── README.md               # This file
├── data/
│   ├── model_C_False.pth       # Trained model (generated)
│   ├── scaler.pkl              # Output scaler (generated)
│   ├── scalerU.pkl             # Input scaler (generated)
│   ├── A_C_False.npy           # A matrix (generated)
│   ├── B_C_False.npy           # B matrix (generated)
│   └── C_C_False.npy           # C matrix (generated)
└── experimental_data/
    ├── T1_ident_1.mat
    ├── T2_ident_1.mat
    ├── T4_ident_1.mat
    ├── u1_ident_1.mat
    ├── u2_ident_1.mat
    ├── u3_ident_1.mat
    ├── T1_ident_2.mat
    ├── T2_ident_2.mat
    ├── T4_ident_2.mat
    ├── u1_ident_2.mat
    ├── u2_ident_2.mat
    └── u3_ident_2.mat
```

## Notes

- The model uses `matrix_C = False`, meaning the decoder is a neural network (MLP) rather than a linear matrix
- All data is automatically scaled/unscaled by the interface functions
- The internal state `x` is maintained as a global variable and updated by `get_x()` and `y_plus()`
- For MPC applications, extract the A and B matrices using `model.A` and `model.B` after calling `init()`

## Citation

If you use this model in your research, please cite the relevant papers on Koopman-based MPC and NeuroMANCER.

## Contact

For questions or issues, please refer to the main project documentation or contact the repository maintainer.
